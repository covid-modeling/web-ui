#!/usr/bin/env node

const fs = require('fs')
const path = require('path')
const https = require('https')
const URL = require('url')
const {DateTime} = require('luxon')
const mysql = require('serverless-mysql')
const d3 = require('d3')
const _ = require('lodash')

require('ts-node').register({
  project: path.join(__dirname, '../tsconfig.json'),
  compilerOptions: {
    module: 'commonjs'
  }
})
const internationalParser = require('../lib/international-interventions-parser')

const cacheDir = process.argv[2]
if (cacheDir === '--help') {
  console.log(`
USAGE

  fetch-recorded-data [cache-directory]

INFO

  Download public datasets for covid-19 cases and policy interventions, format these datasets,
  and write them to MySQL.

  Optionally, pass a path to a directory where downloads should be cached and the results
  should be written as JSON files.

  To store the results in MySQL, set these environment variables:
  DB_HOST, DB_USERNAME, DB_PASSWORD, DB_DATABASE
`)
  process.exit(0)
}

const ecdcCasesURL = `https://opendata.ecdc.europa.eu/covid19/casedistribution/json/`
const covidTrackingURL = 'https://covidtracking.com/api/v1/states/daily.json'
const usInterventionsURL = `https://raw.githubusercontent.com/COVID19StatePolicy/SocialDistancing/master/data/USstatesCov19distancingpolicy.csv`
const internationalSchoolClosuresURL = `https://raw.githubusercontent.com/OxCGRT/covid-policy-tracker/master/data/timeseries/c1_schoolclosing.csv`
const internationalRestrictionsOnGatheringsURL = `https://raw.githubusercontent.com/OxCGRT/covid-policy-tracker/master/data/timeseries/c4_restrictionsongatherings.csv`
const internationalStayAtHomeRequirementsURL = `https://raw.githubusercontent.com/OxCGRT/covid-policy-tracker/master/data/timeseries/c6_stayathomerequirements.csv`

const db = mysql({
  maxRetries: 5,
  config: {
    host: process.env.DB_HOST,
    user: process.env.DB_USERNAME,
    password: process.env.DB_PASSWORD,
    database: process.env.DB_DATABASE,
    ssl: process.env.NODE_ENV === 'production' && {
      ca: fs.readFileSync(
        require.resolve('../lib/BaltimoreCyberTrustRoot.crt.pem'),
        'utf8'
      )
    },
    dateStrings: true
  }
})

async function main() {
  // Fetch the raw data
  const [
    usaCasesJSON,
    ecdcCasesJSON,
    usInterventionsCSV,
    internationalSchoolClosuresCSV,
    internationalRestrictionsOnGatheringsCSV,
    internationalStayAtHomeRequirementsCSV
  ] = await Promise.all([
    fetchCached(covidTrackingURL, cacheDir),
    fetchCached(ecdcCasesURL, cacheDir),
    fetchCached(usInterventionsURL, cacheDir),
    fetchCached(internationalSchoolClosuresURL, cacheDir),
    fetchCached(internationalRestrictionsOnGatheringsURL, cacheDir),
    fetchCached(internationalStayAtHomeRequirementsURL, cacheDir)
  ])

  // Parse the US case data
  const metricsByState = {}
  const usCaseRecords = JSON.parse(usaCasesJSON)
    .sort((a, b) => a.date - b.date)
    .map(row => {
      const regionID = 'US'
      const subregionID = `US-${row.state}`

      // Date is an integer with digits YYYYMMDD
      const dateSQL = DateTime.fromFormat(
        row.date.toString(),
        'yyyyMMdd'
      ).toISODate()

      // Fill in null values with the last non-null value or zero
      const current =
        metricsByState[row.state] || (metricsByState[row.state] = {})
      current.confirmed = row.positive || current.confirmed || 0
      current.recovered = row.recovered || current.recovered || 0
      current.deaths = row.death || current.deaths || 0

      return [
        regionID,
        subregionID,
        dateSQL,
        current.confirmed,
        current.recovered,
        current.deaths
      ]
    })

  // Parse the non-us case data
  const ecdcCases = JSON.parse(ecdcCasesJSON).records
  const worldRows = _.chain(ecdcCases)
    // Group by country
    .groupBy('geoId')
    // Sort the per day entries so that we can calculate the cumulative values correctly.
    .mapValues(v => _.sortBy(v, [v => toISODate(v.dateRep)]))
    .mapValues(v =>
      _.reduce(
        v,
        (acc, o) => {
          const preCumCases =
            acc.length !== 0 ? acc[acc.length - 1].cumCases : 0
          const preCumDeaths =
            acc.length !== 0 ? acc[acc.length - 1].cumDeaths : 0
          // Create cumulative values in addition to the daily values.
          o.cumCases = preCumCases + parseInt(o.cases)
          o.cumDeaths = preCumDeaths + parseInt(o.deaths)
          acc.push(o)
          return acc
        },
        []
      )
    )
    .values()
    .flatten()
    // Remove any US data since we get that from another source.
    .filter(o => o.geoId !== 'US')
    .map(o => [o.geoId, null, toISODate(o.dateRep), o.cumCases, 0, o.cumDeaths])
    .forEach(r => {
      // They use UK as the country code, but we expect GB, so switch them.
      if (r[0] === 'UK') {
        r[0] = 'GB'
      }
    })
    .value()

  const caseRecords = usCaseRecords.concat(worldRows)

  // Parse the US intervention data
  const usInterventionRecords = []
  for (const row of d3.csvParse(usInterventionsCSV)) {
    if (row.StatePolicy && row.StatePostal && row.DateEnacted) {
      const regionId = 'US'
      const subregionId = `US-${row.StatePostal}`
      const policy = row.StatePolicy
      const notes = row.PolicyCodingNotes
      const source = row.PolicySource || null
      const issueDate = row.DateIssued || null
      const startDate = row.DateEnacted
      const easeDate = row.DateEased || null
      const expirationDate = row.DateExpiry || null
      const endDate = row.DateEnded || null
      usInterventionRecords.push({
        regionId,
        subregionId,
        policy,
        notes,
        source,
        issueDate,
        startDate,
        easeDate,
        expirationDate,
        endDate
      })
    }
  }

  // parse the international interventions data
  const internationalSchoolClosures = internationalParser.parseCsv(
    internationalSchoolClosuresCSV,
    'SchoolClose',
    2
  )
  const internationalRestrictionsOnGatherings = internationalParser.parseCsv(
    internationalRestrictionsOnGatheringsCSV,
    'GathRestrict10',
    3
  )
  const internationalStayAtHomeRequirements = internationalParser.parseCsv(
    internationalStayAtHomeRequirementsCSV,
    'StayAtHome',
    2
  )

  const allInterventionRecords = usInterventionRecords
    .concat(internationalSchoolClosures)
    .concat(internationalRestrictionsOnGatherings)
    .concat(internationalStayAtHomeRequirements)
    .filter(row => !!row.startDate)
    .map(row => [
      row.regionId,
      row.subregionId,
      row.policy,
      row.notes,
      row.source,
      row.issueDate,
      row.startDate,
      row.easeDate,
      row.expirationDate,
      row.endDate
    ])

  if (cacheDir) {
    fs.writeFileSync(
      path.join(cacheDir, 'case-data.json'),
      JSON.stringify(caseRecords, null, 2),
      'utf8'
    )

    fs.writeFileSync(
      path.join(cacheDir, 'intervention-data.json'),
      JSON.stringify(allInterventionRecords, null, 2),
      'utf8'
    )
  }

  let isError = false
  try {
    console.log('Inserting case data')

    if (!caseRecords.length) {
      throw new Error('No case data found')
    }

    await db.query('START TRANSACTION')

    // Populate the case_data table
    await db.query('CREATE TABLE case_data_import LIKE case_data')
    await db.query(
      `
        INSERT INTO case_data_import
        (region_id, subregion_id, date, confirmed, recovered, deaths)
        VALUES
        ?
      `,
      [caseRecords]
    )
    console.log(`Saved ${caseRecords.length} records to case_data table...`)
    await db.query(
      'RENAME TABLE case_data TO case_data_old, case_data_import TO case_data'
    )

    await db.query('COMMIT')
  } catch (e) {
    console.error('Failed to insert case data')
    console.error(e)
    isError = true
  } finally {
    await db.query('DROP TABLE IF EXISTS case_data_old')
    await db.query('DROP TABLE IF EXISTS case_data_import')
  }

  try {
    console.log('Inserting interventions data')

    if (!allInterventionRecords.length) {
      throw new Error('No interventions found')
    }

    // INTERVENTIONS DATA
    await db.query('START TRANSACTION')
    // Populate the intervention_data table
    await db.query(
      'CREATE TABLE intervention_data_import LIKE intervention_data'
    )
    await db.query(
      `
        INSERT INTO intervention_data_import
        (
          region_id, subregion_id, policy, notes, source,
          issue_date, start_date, ease_date, expiration_date, end_date
        )
        VALUES
        ?
      `,
      [allInterventionRecords]
    )
    console.log(
      `Saved ${allInterventionRecords.length} records to interventions table...`
    )
    await db.query(
      'RENAME TABLE intervention_data TO intervention_data_old, intervention_data_import TO intervention_data'
    )

    await db.query('COMMIT')
  } catch (e) {
    console.error('Failed to insert interventions data')
    console.error(e)
    isError = true
  } finally {
    await db.query('DROP TABLE IF EXISTS intervention_data_old')
    await db.query('DROP TABLE IF EXISTS intervention_data_import')
  }

  if (isError) {
    console.error('Failed to complete all tasks')
    process.exit(1)
  }
}

async function fetchCached(url, cacheDir) {
  const cacheFilename = path.basename(url)
  const cachePath = cacheDir && path.join(cacheDir, cacheFilename)

  if (cachePath && fs.existsSync(cachePath)) {
    console.log(`Using existing download ${cachePath}...`)
    return fs.readFileSync(cachePath, 'utf8')
  } else {
    console.log(`Downloading from ${url}...`)
    let result = ''
    await new Promise((resolve, reject) => {
      fetch(url)
      function fetch(url) {
        https.get(url, res => {
          // Follow path redirects
          if (res.statusCode === 301 || res.statusCode === 302) {
            const oldURL = URL.parse(url)
            const locationURL = URL.parse(res.headers.location)
            oldURL.path = locationURL.path
            oldURL.pathname = locationURL.pathname
            oldURL.href = null
            const redirectURL = URL.format(oldURL)
            console.log(`Redirected\n  from ${url}\n  to ${redirectURL}...`)
            fetch(redirectURL)
          } else {
            res.on('data', chunk => (result += chunk))
            res.on('end', resolve)
            res.on('error', reject)
          }
        })
      }
    })
    if (cachePath) {
      console.log(`Saving download to ${cachePath}...`)
      fs.writeFileSync(cachePath, result, 'utf8')
    }
    return result
  }
}

function toISODate(dateRep) {
  return DateTime.fromFormat(dateRep, 'dd/MM/yyyy').toISODate()
}

main()
  .then(() => {
    process.exit(0)
  })
  .catch(error => {
    console.error(error)
    process.exit(1)
  })
